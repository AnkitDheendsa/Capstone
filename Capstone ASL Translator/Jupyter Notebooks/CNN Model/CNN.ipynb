{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Logistic Regression ASL Model\n",
        "<br>\n",
        "<br>\n",
        "Ankit Dheendsa - Brainstation\n",
        "<br>\n",
        "September 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal of creating a CNN is to build a deeplearning neural network that can perform ASL translation fast and reliable. We utilize a Convolutional Neural Network due to its ability to process more complex patterns and to show more accurate results despite harder validation sets. Our CNN will utilize the image data we collected for different hand signs instead of numerical representations of the landmark positions. The process of determining which deep learning method to pursue was an iterative one. After running the logistic regression demo it was found that using numerical representations of the hand data would require a lot more processing of the data and mutliple types of averaging techniques would need to be used to combat the constantly changing numerical values when performing a hand sign on a live webcam feed opposed to a picture. As a result a CNN made the most sense as it would be able to comprehend complex hand patterns that best represent the data it has been trained on and therefore would be far more reliable and quicker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Getting started with our CNN we will import all required modules, libraries and frameworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cjAF3KzIay4H"
      },
      "outputs": [],
      "source": [
        "# Required imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image, ImageOps, UnidentifiedImageError\n",
        "from keras.models import load_model\n",
        "from tqdm import tqdm  # Import tqdm for progress bar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will start by getting the path of the data folder, due to different systems and directories we will use the .expanduser() function to get the desktop path and then add in the relative path to avoid bugs and issues when trying to replicate this project on your own machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RvL--HMOb6uH"
      },
      "outputs": [],
      "source": [
        "# Get the absolute path to the \"CNN\" folder on your desktop (sometimes the folder wasnt being accessed via relative path so this gets rid of that uncertainty)\n",
        "# In order to accomplish this we will use the os module\n",
        "desktop_path = os.path.expanduser(\"~/Desktop\")\n",
        "dataset_dir = os.path.join(desktop_path, \"Jupyter Notebooks/CNN Model/Data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will continue by setting the img width and height parameters to standardize the image sizing going into training the model and we will set the batch size to 32 (this value was determined after an iterative process of testing various hyper parameters for optimal results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z7oZ92XHcOt6"
      },
      "outputs": [],
      "source": [
        "# Define image dimensions and batch size (we keep the sizing the same as when the data was created to normalize the images as much as possible)\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "# Here we define our batch size (this is an iterative process that needs to be tested with other hyper parameters for optimal model performance)\n",
        "batch_size = 32 # We found that a batch size of 32 was suitable for our models purpose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Moving on, we will start by creating an image data generator which will be used for data augmentation and pre processing. An image data generator allows us to apply transformations to the image so that the model can learn to distinguish a class despite image altering transformations, thus creating a smarter and more robust model that will respond better to unseen (validation) data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Rimyj6GqcRRj"
      },
      "outputs": [],
      "source": [
        "# Next we will create an ImageDataGenerator for data augmentation and preprocessing\n",
        "# The purpose of this step is to apply various transformations to the image so that the model is more robust and better responds to unseen data that can be encountered later during actual usage\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0,  # This normalizes pixel values to [0, 1]\n",
        "    rotation_range=20,  # Randomly rotate images up to 20 degrees\n",
        "    width_shift_range=0.1,  # Randomly shift the width of the images\n",
        "    height_shift_range=0.1,  # Randomly shift the height of the images\n",
        "    shear_range=0.2,  # Randomly apply shearing transformations\n",
        "    zoom_range=0.2,  # Randomly zoom in on images\n",
        "    horizontal_flip=True,  # Randomly flip images horizontally\n",
        "    fill_mode='nearest'  # Fill in new pixels using the nearest available pixel (good for generalization and pattern recognition)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will continue by creating a train generator which will be used to create a batch of training data from the data folder. This allows us to easily pass this to the CNN to train it effectively. The train and validation generator is where we will utilize our batch size variable that we instantiated earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YP7SW4UPcTqT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 15146 images belonging to 8 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,  # Specify the path to the data directory (Jupyter Notebooks/CNN Model/Data)\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',  # We can use 'categorical' for multi-class classification\n",
        "    subset='training'  # Here we specify that this is the training subset\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will recreate the same step as above by creating a validation generator which will be used to create a batch of validation data from the data folder. This will be passed to the CNN later on when we are testing our model on validation (unseen data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlYAyMrRcWjN"
      },
      "outputs": [],
      "source": [
        "validation_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,  # Specify path to the data directory (Jupyter Notebooks/CNN Model/Data)\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',  # Use 'categorical' for multi-class classification\n",
        "    subset='validation'  # Specify that this is the validation subset\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will get the class labels and their index and print them. This is an important step as a correct label order is required for our labels.txt file so that the CNN can accurately label the predicted class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QYivKhHbca05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'A': 0, 'B': 1, 'C': 2, 'HELLO': 3, 'I': 4, 'K': 5, 'N': 6, 'T': 7}\n"
          ]
        }
      ],
      "source": [
        "# Indicing the class labels/index for ordering purposes\n",
        "class_indices = train_generator.class_indices\n",
        "# Here we print the class indices to see the order (will be important to ensure correct mapping in conjunction with the \"labels.txt\" file)\n",
        "print(class_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can move on to defining the CNN model as well as its convolutional layers, we will instantiate a sequential model type with 3 convolutional 2-D layers along with 3 max-pooling layers to reduce spatial dimensions which would make the model less prone to overfitting and makes it more efficient (time and computationally)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G71gCKHJcdNm"
      },
      "outputs": [],
      "source": [
        "# Here we define the CNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Now we can setup the convolutional and max-pooling layers\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3))) #setting it to have 32 filters with each having a size of (3,3) and the input shape is the images width and height\n",
        "model.add(MaxPooling2D((2, 2))) #After the convolutional layer we add a max pooling layer which reduces the spatial dimensions of the feature maps (makes it more efficient and less prone to overfitting)\n",
        "model.add(Conv2D(64, (3, 3), activation='relu')) #We repeat the same architecture as seen above for 2 more convolutional and max-pooling layers, each new convolutional layer has 2x the filters of the previous\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can continue to flatten the layers into a 1-D vector of connected layers rather than keeping the model 3 dimensional. Here is where we will also incorproate fully connected layers with a number of \"neurons\" whos job is to learn complex patterns with the flattened feature data we created above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-gctHlaXcg6O"
      },
      "outputs": [],
      "source": [
        "# We now flatten layers from the 3-D output gained from the above step (3 convolutional and max-pooling layers) into a 1-D vector (connected layers)\n",
        "model.add(Flatten())\n",
        "\n",
        "# We can now incorporate the fully connected layers\n",
        "model.add(Dense(128, activation='relu')) #Here we add a fully connected/dense layer with 128 \"neurons\" or units that are used for learning complex patterns with the newly flattened feature data\n",
        "model.add(Dropout(0.5))  # Dropout for regularization to help with minimizing overfitting risk (reduces reliance on specific neurons to improve generalization)\n",
        "model.add(Dense(len(train_generator.class_indices), activation='softmax'))  # Number of classes based on folder names are added as neurons using softmax as an activation type (suitable method for mulit-class classification)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can compile our model and incorporate categorical cross entropy for our multi-class classification as well as defining our learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "v_liHqsgcg61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        }
      ],
      "source": [
        "# Next we can compile the model\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',  # We use categorical cross-entropy for multi-class classification\n",
        "    optimizer=Adam(lr=0.001), #We can specify the learning rate\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can continue by training the model using the train generator we instantiated earlier, we will also set the number of epochs (times the model goes through the entire data set) to 15 (this was an iterative process just like the batch size to get as close to 100% accuracy). The expected output will be a list of trials (1 for every epoch) where the model will train and the accuracy goes up over time whilst the loss will go down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lDdkFbB-cmQg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "473/473 [==============================] - 163s 344ms/step - loss: 0.7398 - accuracy: 0.7326\n",
            "Epoch 2/15\n",
            "473/473 [==============================] - 166s 350ms/step - loss: 0.3271 - accuracy: 0.8820\n",
            "Epoch 3/15\n",
            "473/473 [==============================] - 166s 350ms/step - loss: 0.2422 - accuracy: 0.9148\n",
            "Epoch 4/15\n",
            "473/473 [==============================] - 165s 349ms/step - loss: 0.1995 - accuracy: 0.9320\n",
            "Epoch 5/15\n",
            "473/473 [==============================] - 166s 351ms/step - loss: 0.1676 - accuracy: 0.9404\n",
            "Epoch 6/15\n",
            "473/473 [==============================] - 167s 352ms/step - loss: 0.1477 - accuracy: 0.9511\n",
            "Epoch 7/15\n",
            "473/473 [==============================] - 167s 352ms/step - loss: 0.1320 - accuracy: 0.9536\n",
            "Epoch 8/15\n",
            "473/473 [==============================] - 167s 353ms/step - loss: 0.1197 - accuracy: 0.9594\n",
            "Epoch 9/15\n",
            "473/473 [==============================] - 196s 414ms/step - loss: 0.1113 - accuracy: 0.9620\n",
            "Epoch 10/15\n",
            "473/473 [==============================] - 167s 352ms/step - loss: 0.1127 - accuracy: 0.9596\n",
            "Epoch 11/15\n",
            "473/473 [==============================] - 168s 355ms/step - loss: 0.0839 - accuracy: 0.9711\n",
            "Epoch 12/15\n",
            "473/473 [==============================] - 167s 353ms/step - loss: 0.0892 - accuracy: 0.9708\n",
            "Epoch 13/15\n",
            "473/473 [==============================] - 594s 1s/step - loss: 0.0783 - accuracy: 0.9737\n",
            "Epoch 14/15\n",
            "473/473 [==============================] - 872s 2s/step - loss: 0.0763 - accuracy: 0.9746\n",
            "Epoch 15/15\n",
            "473/473 [==============================] - 902s 2s/step - loss: 0.0754 - accuracy: 0.9741\n"
          ]
        }
      ],
      "source": [
        "# Now we are able to train the model using the train_generator we instantiated earlier\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=15,  # We adjust the number of epochs (same as batch size) iteratively to determine the best value for model training\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see from our results that 15 epochs was a suitable setting as we ended up with a 97.41% accuracy rate by the end of the training phase (the model had hit 99% at one point during the 15th epoch so it wouldnt be wise to continue iterating as we might risk overfitting and the accuracy going down)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can save the model as a .h5 file so that we may import it into our demo script later on to use in a practical live setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iguG4-Xwcorv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Finally we can save the trained model\n",
        "# We wont run this cell again since we already ran it the file was saved in the directory\n",
        "model.save(\"asl_classifier.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a statistical metric we will create a custom classification table that will measure confidence scores of the model as well as the accuracy in predicting the class. The reason we incorporate confidence scores is to be able to determine where issues may lie within the model, for if the model is confident in its prediction (99% for example) but predicts wrong, it may be that our dataset is fault and needs to be looked at again rather than attempting to \"fix\" the model. The expected output for this code chunk is a table that prints out the class name, the average confidence score of the model when predicting within that class, and the accuracy of the model.\n",
        "<br>\n",
        "<br>\n",
        "We will accomplish this by iterating through our Validation Data set which consists of 500 images for each class. All images have not been seen by the model before which will allow us to get a good idea of how the model would perform in a real life scenario. We will iterate through the validation folder into the subfolders of each class that contain the image data, we will then pass each image iteratively through the model and have it make a prediction. We will then record the confidence score of that prediction and compare the predicted class with the actual class of the image to determine if it was accurate or not. The final scores are tallies up and divided to get our averaged values.\n",
        "<br>\n",
        "<br>\n",
        "As this can be a somewhat lengthy process (2 min 3 sec on average), we will incorporate a progress bar using the tqdm library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [02:05<00:00, 15.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class | Average Confidence | Accuracy\n",
            "I | 0.9999 | 1.0000\n",
            "N | 0.9977 | 0.9680\n",
            "T | 0.9984 | 0.9782\n",
            "A | 0.9994 | 0.9940\n",
            "Hello | 1.0000 | 1.0000\n",
            "C | 1.0000 | 1.0000\n",
            "B | 1.0000 | 1.0000\n",
            "K | 1.0000 | 1.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# We will disable scientific notation for clarity (making it easier to read for those not familiar with scientific notation)\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Here we will load the model that we created and saved\n",
        "model = load_model(\"asl_classifier.h5\", compile=False)\n",
        "\n",
        "# We will continue to load the labels of the validation data set\n",
        "class_names = open(\"validation_labels.txt\", \"r\").readlines()\n",
        "\n",
        "# We will instantiate an empty lis to store results\n",
        "results = []\n",
        "\n",
        "# Here we specify the path to the main directory containing subfolders (each subfolder is a class)\n",
        "main_directory = \"Validation Data\"\n",
        "\n",
        "# Now we will get a list of the subfolders \n",
        "class_folders = [class_name for class_name in os.listdir(main_directory) if os.path.isdir(os.path.join(main_directory, class_name))]\n",
        "\n",
        "# We can now use tqdm to create a progress bar for convenience and better understanding of the iterative process and its timing\n",
        "with tqdm(total=len(class_folders)) as pbar:\n",
        "    # We loop through subfolders \n",
        "    for class_name in class_folders:\n",
        "        class_directory = os.path.join(main_directory, class_name)\n",
        "\n",
        "        # Here we instantiate empty variables that will hold our results when they are calcualted\n",
        "        correct_predictions = 0\n",
        "        total_confidence = 0.0\n",
        "        total_images = 0\n",
        "\n",
        "        # Now we loop through images in the class directory so that they can be passed to the model\n",
        "        for image_filename in os.listdir(class_directory):\n",
        "            image_path = os.path.join(class_directory, image_filename)\n",
        "\n",
        "            try:\n",
        "                # We preprocess the image similar to how we did with our generators and in the data collection python script for generalization (so that the images are more readable to the model and what its used to)\n",
        "                image = Image.open(image_path).convert(\"RGB\")\n",
        "                size = (224, 224)\n",
        "                image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)\n",
        "                image_array = np.asarray(image) #we store the image as a numpy array so that we can apply normalization methods\n",
        "                normalized_image_array = (image_array.astype(np.float32) / 127.5) - 1 #now we can normalize the previously created image array\n",
        "                data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32) #we define the shape and the data type (set to float) so that there are no errors\n",
        "                data[0] = normalized_image_array\n",
        "\n",
        "                # Now we can predict the class label for the image using our CNN\n",
        "                prediction = model.predict(data, verbose=0) #we set verbose to 0 so that each iteration isnt printed in the terminal (keeps the output clean and more readable)\n",
        "                index = np.argmax(prediction)\n",
        "                confidence_score = prediction[0][index] #we can keep track of the confidence score by indexing the prediction\n",
        "\n",
        "                # Check if the prediction matches the actual class name\n",
        "                if confidence_score > 0.99 and class_name == class_name:\n",
        "                    correct_predictions += 1 #if confidence is above 99% and the class names match we consider it accurate (this decreases likelihood of false positives)\n",
        "\n",
        "                # We can now add to our tallies\n",
        "                total_confidence += confidence_score\n",
        "                total_images += 1\n",
        "\n",
        "            # Here we can throw an exception if an image is unreadble or cannot be opened due to some kind of unforseen error\n",
        "            except (UnidentifiedImageError, OSError):\n",
        "                # Skip images that cannot be identified or opened\n",
        "                continue\n",
        "\n",
        "        # Calculate accuracy and average confidence for the class\n",
        "        if total_images > 0:\n",
        "            accuracy = correct_predictions / total_images\n",
        "            average_confidence = total_confidence / total_images\n",
        "        else:\n",
        "            accuracy = 0.0\n",
        "            average_confidence = 0.0\n",
        "\n",
        "        # Now we can append results to the list\n",
        "        results.append((class_name, average_confidence, accuracy))\n",
        "        \n",
        "        # Here we update the progress bar after a class has been finished testing\n",
        "        pbar.update(1)\n",
        "\n",
        "# Print the final summary table\n",
        "print(\"Class | Average Confidence | Accuracy\")\n",
        "for result in results:\n",
        "    class_name, avg_confidence, accuracy = result\n",
        "    print(f\"{class_name} | {avg_confidence:.4f} | {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see with our final results that the model performs exceptionally well with the lowest accuracy score being 96% for the letter 'N'. We notice a major trend here however with the classes that had the lowest accuracy scores. If we pay attention to the letters N,T and A we see all the accuracy scores being less than 100% whilst all the other classes achieved 100% accuracy.\n",
        "<br>\n",
        "<br>\n",
        "We can credit these results due to the nature of the hand signs themselves. The letters N,T and A are all extremely similar with only minor adjustments to differentiate them all, whilst all the classes that scored 100% accuracy are very distinct signs that typically require the spread of your fingers to be further from the palm (refer to an ASL chart for better understanding).\n",
        "<br>\n",
        "<br>\n",
        "As a result, without the need of a confusion matrix we can determine the reasoning for these model results and a possible solution. In terms of training the model to better differentiate between more similar signs, we can simply increase our dataset size for those classes. If that doesnt work entirely our next plan of action would be to optimize the model specifically for those classes as it takes less resources for the model to translate the classes with 100% accuracy we can dedicate a certain amount of computational power to specifically be good at differentiating minor pattern changes with signs that are hyper similar."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
